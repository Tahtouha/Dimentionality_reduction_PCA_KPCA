# -*- coding: utf-8 -*-
"""dim_reduction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQ0AgS1D7ep28dsugw6mJeNZPUo2BjCa

The goal of this TD is to understand the difference between PCA and Kernel based PCA

To perform a PCA based dimension reduction, we need:
<p> 1. to compute the covariance matrix <b> C </b> of the original data <b> X </b>.</p>
<p> 2. to compute the the eigendecomposition of the computed matrix.</p>
<p> 3. to sort the eigen values according to a decreasing order.</p>
<p> 4. to construct the projection matrix <b> W </b> of the k first eigen values.</p>
<p> 5. to transform the data into the projected space $X_{pca} = W^TX $ </p>
"""

import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, random_state=123)

plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)
plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)

plt.title('A nonlinear 2Ddataset')
plt.ylabel('y coordinate')
plt.xlabel('x coordinate')

# perform PCA based reduction on this data
# keep only the first leading eigen value
# plot the resulting points
import numpy as np

def PCA(X , num_components):
     
    #pour le calcule de covariance
    X_meaned = X - np.mean(X , axis = 0)
     
    #Calcule de covariance
    cov_mat = np.cov(X_meaned , rowvar = False)
     
    #2. compute the the eigendecomposition of the computed matrix.
    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)
     
    #sort the eigen values according to a decreasing order.
    sorted_index = np.argsort(eigen_values)[::-1]
    sorted_eigenvalue = eigen_values[sorted_index]
    sorted_eigenvectors = eigen_vectors[:,sorted_index]
     
    #taking subset
    eigenvector_subset = sorted_eigenvectors[:,0:num_components]
     
    #Data transforamtion projection
    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()



    plt.scatter(X_reduced[y==0], (-eigenvector_subset[0] / eigenvector_subset[1] ) * X_reduced[y==0], color='red', alpha=0.5)
    plt.scatter(X_reduced[y==1], (-eigenvector_subset[0] / eigenvector_subset[1] ) * X_reduced[y==1], color='blue', alpha=0.5)
     
    return X_reduced, eigenvector_subset

x_Reduce, eigenvector_subset  = PCA(X,1)

# perform linear classification using your favorite linear classifier

n_samples = 100

#here we set the binary claassifier threshhomd, if the value is greater then 0.5 we set it to 1 otherwise we set it to
threshold = lambda x: 1 if x>0.5 else 0

#using binary classification we have:

def Binary_Linear_classifier(samples, labels, alpha = 0.01):
  n_samples = samples.shape[0]
  samples = np.concatenate((np.ones((n_samples,1)),samples),axis = 1)
  n_features = samples.shape[1]
  cont = 0
  W = np.random.rand(1,n_features)
  while (cont < 1000):
    for i in range(n_samples):
      y_hat = threshold( np.dot(W,samples[i,:].T) )
      W = W + alpha*(labels[i]-y_hat)*samples[1,:]
    cont +=1
  return W

W = Binary_Linear_classifier(x_Reduce,y)

Xnew = np.concatenate((np.ones((n_samples,1)),x_Reduce),axis = 1)

plt.scatter(x_Reduce[y==0], (-eigenvector_subset[0] / eigenvector_subset[1] ) * x_Reduce[y==0], color='red', alpha=0.5)
plt.scatter(x_Reduce[y==1], (-eigenvector_subset[0] / eigenvector_subset[1] ) * x_Reduce[y==1], color='blue', alpha=0.5)

plt.plot(x_Reduce,np.matmul(Xnew,np.transpose(W)),'m')

"""<p> Kernel functions and the kernel trick: </p>
The basic idea to deal with inseparable data using linear classifiers is to project it onto a higher dimensional space where it becomes linearly separable.
<p> To do so, we: </p>
<p> 1. compute the kernel matrix <b> K </b> using RBF kernel for instance $exp(âˆ’\gamma||x_i-x_j||^2)$ </p>
<p> 2. Eigendecompose of the kernel matrix. <b> K </b> </p>
<p> 3. to transform the data into the projected space </p>
"""

from scipy.spatial.distance import pdist, squareform
from scipy import exp
from scipy.linalg import eigh
import numpy as np 

def rbf_kernel_pca(X, gamma, n_components):
   

    sq_dists = pdist(X, 'sqeuclidean')    
    mat_sq_dists = squareform(sq_dists)    
    K = exp(-gamma * mat_sq_dists)    
    N = K.shape[0]
    one_n = np.ones((N,N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)  
    eigvals, eigvecs = eigh(K)
    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]    
    X_pc = np.column_stack([eigvecs[:, i]
    for i in range(n_components)])    
    return X_pc, eigvecs

X_kpca, eigvecs = rbf_kernel_pca(X, gamma=10, n_components=1)

threshold = lambda x: 1 if x>0.5 else 0
W = Binary_Linear_classifier(X_kpca,y)

Xnew = np.concatenate((np.ones((n_samples,1)),X_kpca),axis = 1)

plt.scatter(X_kpca[y==0], (-eigenvector_subset[0] / eigenvector_subset[1] ) * X_kpca[y==0], color='red', alpha=0.5)
plt.scatter(X_kpca[y==1], (-eigenvector_subset[0] / eigenvector_subset[1] ) * X_kpca[y==1], color='blue', alpha=0.5)


plt.plot(X_kpca,np.matmul(Xnew,np.transpose(W)),'m')

